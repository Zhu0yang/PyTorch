{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc7ee58",
   "metadata": {},
   "source": [
    "# 第二节    神经网络实现分类任务\n",
    "\n",
    "- 网络基本构建与训练方法，常用函数解析\n",
    "\n",
    "- torch.nn.functional模块\n",
    "\n",
    "- nn.Module模块\n",
    "\n",
    "#### Mnist分类任务\n",
    "\n",
    "1. 注意数据需转换成tensor才能参与后续建模训练\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "```\n",
    "---\n",
    "2. 如果模型有可学习的参数，最好用nn.Module，其他情况nn.functional相对更简单一些\n",
    "```python\n",
    "# nn.functional()\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb.mm(weights) + bias\n",
    "\n",
    "bs = 64\n",
    "xb = x_train[0:bs]  # a mini-batch from x\n",
    "yb = y_train[0:bs]\n",
    "\n",
    "weights = torch.randn([784, 10], dtype = torch.float,  requires_grad = True) \n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "3. 创建一个model来更简化代码\n",
    "\n",
    "- 必须继承nn.Module且在其构造函数中需调用nn.Module的构造函数\n",
    "- 无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播\n",
    "- Module中的可学习参数可以通过named_parameters()或者parameters()返回迭代器\n",
    "\n",
    "```python\n",
    "# nn.Module()\n",
    "from torch import nn\n",
    "\n",
    "class Mnist_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(748,128)\n",
    "        self.hidden2 = nn.Linear(128,256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.dropout(x)     # 全连接层后可以加上dropout\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = Mnist_NN()\n",
    "\n",
    "# print(net)\n",
    "# for name, parameter in net.named_parameters():\n",
    "#     print(name, parameter, parameter.size())\n",
    "\n",
    "```\n",
    "---\n",
    "4. 使用TensorDataset和DataLoader导入数据  \n",
    "DataLoader用于数据“打包”\n",
    "```python\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)    \n",
    "# 训练集打乱顺序，shuffle = True\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n",
    "\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "5. 训练网络\n",
    "- 一般在训练模型时加上model.train()，这样会正常使用Batch Normalization和 Dropout\n",
    "- 测试的时候一般选择model.eval()，这样就不会使用Batch Normalization和 Dropout\n",
    "\n",
    "```python\n",
    "# 训练网络\n",
    "import numpy as np\n",
    "def fit(steps, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for step in range(steps):       \n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print('当前step:'+str(step), '验证集损失：'+str(val_loss))\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_NN()\n",
    "    return model, optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()     # 算反向梯度\n",
    "        opt.step()          # 梯度下降优化\n",
    "        opt.zero_grad()     # 注意还原\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "\n",
    "# 主程序：\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(25, model, loss_func, opt, train_dl, valid_dl)\n",
    "\n",
    "# epoch：整轮，训练一次整个训练集\n",
    "# bitch：小批，每单次训练的数据个数\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# 准确率\n",
    "correct = 0\n",
    "total = 0\n",
    "for xb, yb in valid_dl:\n",
    "    outputs = model(xb)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += yb.size(0)\n",
    "    correct += (predicted == yb).sum().item()\n",
    "\n",
    "print('验证集准确率: %d %%' % (100 * correct / total))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507fb9a",
   "metadata": {},
   "source": [
    "# 第三节    神经网络实现回归任务\n",
    "\n",
    "#### 气温预测回归\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "features = pd.read_csv('temps.csv')\n",
    "\n",
    "\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
